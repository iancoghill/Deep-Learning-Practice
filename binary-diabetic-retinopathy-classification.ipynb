{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Diabetic Retinopathy Binary Classification\n#### Using transfer learning with the pretrained VGG16 model\n"},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from PIL import Image\nimport pathlib\nimport math\nfrom glob import glob\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras import backend as K\nfrom keras.models import Model\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers import Dense, Dropout, Flatten, Input, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import load_img,img_to_array,ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Load Images from Each Folder\n#Reduce Dataset Size by Factor (to avoid running out of RAM)\nReductionFactor = 20\n\n# Base Path\npath = ('../input/diabetic-retinopathy-2015-data-colored-resized/colored_images/colored_images/')\n\n# Image Reading Function\ndef read_images(path,num_img):\n    array=np.zeros((num_img,224,224,3))\n    i=0\n    for img in os.listdir(path):\n        if (i == num_img):\n            break\n        img_path=path + \"/\" + img\n        img=Image.open(img_path,mode=\"r\")\n        data=np.asarray(img,dtype=\"uint8\")\n        array[i]=data\n     \n        i+=1\n    return array\n\n# No DR\nnoDR_dir = os.path.join(path, 'No_DR')\nn_noDR=math.ceil(len(glob(os.path.join(path, 'No_DR/*')))/ReductionFactor)\nX_noDR=read_images(noDR_dir,n_noDR)\nX_noDR=X_noDR.astype(np.uint8)\nprint(\"No DR Shape: \",X_noDR.shape)\n\n# Mild DR\nmildDR_dir = os.path.join(path, 'Mild')\nn_mildDR=math.ceil(len(glob(os.path.join(path, 'Mild/*')))/ReductionFactor)\nX_mildDR=read_images(mildDR_dir,n_mildDR)\nX_mildDR=X_mildDR.astype(np.uint8)\nprint(\"Mild DR Shape: \",X_mildDR.shape)\n\n# Moderate DR\nmodDR_dir = os.path.join(path, 'Moderate')\nn_modDR=math.ceil(len(glob(os.path.join(path, 'Moderate/*')))/ReductionFactor)\nX_modDR=read_images(modDR_dir,n_modDR)\nX_modDR=X_modDR.astype(np.uint8)\nprint(\"Moderate DR Shape: \",X_modDR.shape)\n\n# Proliferate DR\nproDR_dir = os.path.join(path, 'Proliferate_DR')\nn_proDR=math.ceil(len(glob(os.path.join(path, 'Proliferate_DR/*')))/ReductionFactor)\nX_proDR=read_images(proDR_dir,n_proDR)\nX_proDR=X_proDR.astype(np.uint8)\nprint(\"Proliferate DR Shape: \",X_proDR.shape)\n\n# Severe DR\nsevDR_dir = os.path.join(path, 'Severe')\nn_sevDR=math.ceil(len(glob(os.path.join(path, 'Severe/*')))/ReductionFactor)\nX_sevDR=read_images(sevDR_dir,n_sevDR)\nX_sevDR=X_sevDR.astype(np.uint8)\nprint(\"Severe DR Shape: \",X_sevDR.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show an Image from Each Category\nfig=plt.figure()\nfig.suptitle('Left to Right: No DR, Mild, Moderate, Proliferate, Severe')\n\n# No DR\nfig.add_subplot(1,5,1)\nplt.imshow(X_noDR[5])\nplt.axis(\"off\")\n\n# Mild DR\nfig.add_subplot(1,5,2)\nplt.imshow(X_mildDR[5])\nplt.axis(\"off\")\n\n# Moderate DR\nfig.add_subplot(1,5,3)\nplt.imshow(X_modDR[5])\nplt.axis(\"off\")\n\n# Proliferate DR\nfig.add_subplot(1,5,4)\nplt.imshow(X_proDR[5])\nplt.axis(\"off\")\n\n# Severe DR\nfig.add_subplot(1,5,5)\nplt.imshow(X_sevDR[5])\nplt.axis(\"off\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Combine Arrays to Single X Array\n# Concatenate\nX=np.concatenate((X_noDR,X_mildDR,X_modDR,X_proDR,X_sevDR),axis=0)\nprint(\"X Shape: \",X.shape)\n\n# Delete Arrays to Save RAM\ndel X_noDR, X_mildDR, X_modDR, X_proDR, X_sevDR","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Normalise RGB Values to be in Range 0-1\nX = X / 255.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Create Labels Array Y\n# No DR\nzeros=np.zeros(n_noDR)\n\n# DR\nones=np.ones(n_mildDR+n_modDR+n_proDR+n_sevDR)\n\nY = np.concatenate((zeros,ones),axis=0)\nprint(\"Y Shape: \",Y.shape)\n## Create Class Weights Dict\nweights={0: (len(zeros)/(len(zeros)+len(ones))), 1: (len(ones)/(len(zeros)+len(ones)))}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Shuffle Data\nX, Y = shuffle(X, Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Split into Training, Validation and Test (and one-hot-encode labels arrays)\n# Split\n(X_train, X_TestAndVal, Y_train, Y_TestAndVal) = train_test_split(X, Y, test_size=0.20)\n(X_val, X_test, Y_val, Y_test) = train_test_split(X_TestAndVal, Y_TestAndVal, test_size=0.50)\nprint(\"X_train Shape: \",X_train.shape)\nprint(\"X_val Shape: \",X_val.shape)\nprint(\"X_test Shape: \",X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show How Many Images of Each Category are in Training Set\nsns.countplot(Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show How Many Images of Each Category are in Validation Set\nsns.countplot(Y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Show How Many Images of Each Category are in Test Set\nsns.countplot(Y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X, Y, X_TestAndVal, Y_TestAndVal","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"pretrained_model = VGG16(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]), include_top=False, weights=\"imagenet\")\n#pretrained_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for layer in pretrained_model.layers[:15]:\n    layer.trainable = False\n \nfor layer in pretrained_model.layers[13:]:\n    layer.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"layer_dict = dict([(layer.name, layer) for layer in pretrained_model.layers])\n\nx = layer_dict['block5_pool'].output\n\nx = Flatten()(x)\n#Fully connected layer\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.2)(x)\n#this is the final layer so the size of output in this layer is equal to the number of class in our problem\nx = Dense(1, activation='sigmoid')(x)\n#create the new model\ncustom_model = Model(pretrained_model.input, outputs=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"custom_model.compile(optimizer='adam', loss = 'BinaryCrossentropy', metrics=['AUC'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train the Trainable Layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nHistory = custom_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=15, batch_size=200, class_weight = weights)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplot(1,2,1)\nplt.plot(History.history[\"loss\"],label=\"train loss\")\nplt.plot(History.history[\"val_loss\"],label=\"val loss\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(20,10))\nplt.subplot(1,2,2)\nplt.plot(History.history[\"accuracy\"],label=\"train accuracy\")\nplt.plot(History.history[\"val_accuracy\"],label=\"val accuracy\")\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Performance on Test Set"},{"metadata":{"trusted":true},"cell_type":"code","source":"## Test Accuracy\nscore, acc = custom_model.evaluate(X_test, Y_test,verbose = 0)\nprint('Test score:', score)\nprint('Test accuracy:', acc)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Confusion Matrix\npredictions = custom_model.predict(X_test)\nprint(predictions)\nprint(Y_test)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}